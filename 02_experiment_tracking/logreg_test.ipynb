{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "# Scaling\n",
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook you will see a short example of how to select a model, scale your data and tune the hyperparamters of your models using grid or random search.  \n",
    "\n",
    "We will use the titanic dataset.  \n",
    "Since you've already worked your way through the steps of exploring and cleaning the data as well as selecting proper features for modelling in another notebook, we will skip this part here and use the **preprocessed data** from the logistic regression notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "# set seaborn plot style\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "RSEED = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data \n",
    "df = pd.read_csv('data/titanic_preprocessed.csv')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#... or we could reload the stored titanic_dmy from the other notebook\n",
    "# %store -r titanic_dmy\n",
    "# titanic_dmy.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But in this notebook, we will use the preprocessed data from the titanic_preprocessed.csv!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing data\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test-Split\n",
    "\n",
    "Train-Test-Split splits arrays or matrices, also for example our dataframe, into random train and test subsets.  \n",
    "The main idea of splitting the dataset into a validation set is to prevent our model from overfitting i.e., the model becomes really good at classifying the samples in the training set but cannot generalize and make accurate classifications on the data it has not seen before.  \n",
    "\n",
    "We will define the target and predictors and split our dataset into a train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define predictors and target\n",
    "y = df.Survived\n",
    "X = df.drop('Survived', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Y\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check X\n",
    "X.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test-split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for set in (X_train, X_test, y_train, y_test):\n",
    "    print(set.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:05:29.894968Z",
     "start_time": "2020-02-11T09:05:29.885248Z"
    }
   },
   "source": [
    "## Logistic Regression in scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import Logistic Regression classifier from sklearn\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, how easy is it to make some predictions now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression() # instantiate a sklearn logistic regression classs\n",
    "classifier.fit(X_train, y_train) # fit the classifier/model on our train data \n",
    "y_prediction = classifier.predict(X_test) # use the fit model to predict on our test data \n",
    "\n",
    "#have a look at the predicitons\n",
    "y_prediction[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model performance metrics\n",
    "example 1: confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T11:50:11.789405Z",
     "start_time": "2020-06-30T11:50:11.776869Z"
    }
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_prediction, labels=[0,1]) # assign a confusion matrix that compares test data and predictions \n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "group_counts = ['{0:0.0f}'.format(value) for value in\n",
    "                cm.flatten()]\n",
    "group_percentages = ['{0:.2%}'.format(value) for value in\n",
    "                     cm.flatten()/np.sum(cm)]\n",
    "labels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in\n",
    "          zip(group_names,group_counts,group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "sns.heatmap(cm, annot=labels, fmt='', cmap='Blues');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this labels parameter sequence, the matrix horizontally reads the \"predicted\" and vertically the \"actual\" labels,  \n",
    "so in the first line True Negatives, False Positives and in the second line False Negatives and True Positives.  \n",
    "The results from the confusion matrix are telling us that 97 and 43 are the number of correct predictions. 13 and 25 are the number of incorrect predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn classifikation report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T11:50:11.820196Z",
     "start_time": "2020-06-30T11:50:11.796918Z"
    }
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This line of code generates and prints a detailed report summarizing the performance of a classification model.  \n",
    "It compares the actual labels (ground truth) in your test dataset (y_test) with the predictions made by your model (y_prediction).  \n",
    "The report provides key metrics that help you understand how well your model is classifying different classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Precision__ is the accuracy of positive predictions:  \n",
    "Precision = TP/(TP + FP)  \n",
    "  \n",
    "__Recall__ tells you what percent of the positive cases did you in fact catch?  \n",
    "The fraction of positives that were correctly identified:  \n",
    "Recall = TP/(TP+FN)\n",
    "\n",
    "__Accuracy__ is the overall accuracy of your model.  \n",
    "It's the percentage of samples that were correctly classified across all classes.\n",
    "Accuracy = (TP+TN)/Total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let us learn about Classification Report: https://muthu.co/understanding-the-classification-report-in-sklearn/**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. import all the classifiers you want to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all the classifiers you want to evaluate\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. append them to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC(gamma='auto')))\n",
    "models.append(('SGD', SGDClassifier(random_state=RSEED)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. iterate over the list and get a performance metric for every model  \n",
    "(here, we chose \"accuracy\", the ratio of the number of correctly classified cases to the total of cases under evaluation - but it could be based on every other perfomance metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "\tkfold = StratifiedKFold(n_splits=10, random_state=1, shuffle=True)\n",
    "\tcv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring='accuracy')\n",
    "\tresults.append(cv_results)\n",
    "\tnames.append(name)\n",
    "\tprint('%s: %f (%f)' % (name, cv_results.mean(), cv_results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these results, we move on with the Linear Regression Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Features Scaling\n",
    "\n",
    "Often the input features of your model have different units which means that the variables also have different scales. While some model types (e.g. tree-based models like decision tree or random forest) are unaffected by the scale of numerical input variables, many machine learning algorithms including for example algorithms using distance measures (e.g. KNN, SVM) perform better when the input features are scaled to a specific range. \n",
    "\n",
    "The most popular techniques for scaling are **normalization** and **standardization**. \n",
    "\n",
    "Chech the [link](https://machinelearningmastery.com/standardscaler-and-minmaxscaler-transforms-in-python/) for further info. \n",
    "\n",
    "![scaling](images/normalization_vs_standardization.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we have a look at the different methods, \n",
    "# we have to define which columns we want to scale.\n",
    "display(df.describe().round(2))\n",
    "col_scale = ['Age', 'SibSp', 'Parch', 'Fare']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Standardization \n",
    "\n",
    "In order to standardize a dataset it is necessary to rescale the distribution of values so that the mean of observed values is 0 and the standard deviation is 1. You can think of it as subtracting the mean value or centering the data. \n",
    "Sklearn provides us for this case with the [Standard scaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html).\n",
    "\n",
    "A value is standardized as follows: \n",
    "\n",
    "\n",
    "$ x_{scaled} = \\frac{x – \\mu}{\\sigma}  $, where \n",
    "\n",
    "$ \\mu = \\frac{\\sum{x}}{m} $ is the mean, where m is the number of observations\n",
    "\n",
    "$ \\sigma = \\sqrt{ \\frac{\\sum{ (x – \\mu)^2 }}{m}} $ is the standard deviation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling with standard scaler\n",
    "# the fit part method is calculating the mean and the variance of the data\n",
    "# fit_transform applies this to transform all the features in respect to that values\n",
    "# transform applies this to new data in respect to that already learned values, not the new data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train[col_scale])\n",
    "X_test_scaled = scaler.transform(X_test[col_scale])\n",
    "\n",
    "# Concatenating scaled and dummy columns \n",
    "X_train_preprocessed = np.concatenate([X_train_scaled, X_train.drop(col_scale, axis=1)], axis=1)\n",
    "X_test_preprocessed = np.concatenate([X_test_scaled, X_test.drop(col_scale, axis=1)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_preprocessed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data normalization \n",
    "\n",
    "Normalizing the data means to rescale it from the original range so that all values lie within the new range of 0 and 1.\n",
    "We can easily do this by using the [Min-Max-Scaler](https://scikitlearn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) from sklearn. This scaler transforms the feature(s) by scaling it(them) to a given range (default range is 0 to 1). \n",
    "\n",
    "A value is normalized as follows: \n",
    "\n",
    "$ x_{scaled} = \\frac{x – x_{min}}{x_{max} – x_{min}} $\n",
    "\n",
    "(Where the min and max values pertain to the value x being normalized, from your **train** dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling with MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define predictors and target\n",
    "y2 = df.Survived\n",
    "X2 = df.drop('Survived', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test-split\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2, random_state=42, stratify=y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we have a look at the different methods, \n",
    "# we have to define which columns we want to scale.\n",
    "col_scale = ['Age', 'SibSp', 'Parch', 'Fare']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling with minmax scaler\n",
    "mmscaler = MinMaxScaler()\n",
    "X2_train_scaled = mmscaler.fit_transform(X2_train[col_scale])\n",
    "X2_test_scaled = mmscaler.transform(X2_test[col_scale])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating scaled and dummy columns\n",
    "X2_train_preprocessed = np.concatenate([X2_train_scaled, X2_train.drop(col_scale, axis=1)], axis=1)\n",
    "X2_test_preprocessed = np.concatenate([X2_test_scaled, X2_test.drop(col_scale, axis=1)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"test\", pd.DataFrame(X2_train_preprocessed).describe())\n",
    "print(\"---\")\n",
    "print(\"train\", pd.DataFrame(X2_test_preprocessed).describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Predictive Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will evaluate our model performance with a quick and more reliable way using sklearn's [cross_val_score()](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) which implements K-fold cross validation. When training a model based on train and test split we only have one experiment. Can we really trust one experiment? \n",
    "\n",
    "Think of [K-fold cross validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)) as doing K experiments and then taking the average error. It is still not perfect but better than 1 experiment which can randomly turn out to be really good. \n",
    "\n",
    "Whenever we have K, comes the question about the value of K.. common values are between 5 and 10 and you need to take into account the technical limitations: dataset size, compute power and available memory and time. CV takes time on large datasets.\n",
    "\n",
    "\n",
    "![cv](images/cross_validation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression Classifier - unscaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and evaluate model without hyperparameter tuning using cross validation and unscaled data \n",
    "logreg_classifier = LogisticRegression()\n",
    "scores = cross_val_score(logreg_classifier, X_train, y_train, cv=5, n_jobs=-1, verbose=5) \n",
    "\n",
    "# Evaluation \n",
    "print('Score (unscaled):', round(scores.mean(), 4));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the scores and average score\n",
    "plt.axhline(y=scores.mean(), color='y', linestyle='-')\n",
    "sns.barplot(x=[1,2,3,4,5],y=scores).set_title('Scores of the K-Folds Models - unscaled data');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression Classifier - standardized scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and evaluate model using cross validation and scaled data \n",
    "logreg_scaled = LogisticRegression()\n",
    "scores_scaled_std = cross_val_score(logreg_scaled, X_train_preprocessed, y_train, cv=5, n_jobs=-1)\n",
    "\n",
    "# Evaluation\n",
    "print('Score (scaled):', round(scores_scaled_std.mean(), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.axhline(y=scores_scaled_std.mean(), color='y', linestyle='-')\n",
    "sns.barplot(x=[1,2,3,4,5],y=scores_scaled_std).set_title('Scores of the K-Folds Models - standardized data');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model errors on standardized features have a slightly bigger standard deviation than on non-scaled features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression Classifier - normalized scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and evaluate model using cross validation and scaled data \n",
    "log_reg_scaled = LogisticRegression()\n",
    "scores_scaled_norm = cross_val_score(log_reg_scaled, X2_train_preprocessed, y_train, cv=5, n_jobs=-1, scoring='accuracy')\n",
    "# If \"scoring\"=None, the estimator’s default scorer (if available) is used.\n",
    "\n",
    "# Evaluation\n",
    "print('Score (scaled):', round(scores_scaled_norm.mean(), 4))\n",
    "\n",
    "plt.axhline(y=scores_scaled_norm.mean(), color='y', linestyle='-')\n",
    "sns.barplot(x=[1,2,3,4, 5],y=scores_scaled_norm).set_title('Scores of the K-Folds Models - normalized data');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Score (unscaled):', round(scores.mean(), 4))\n",
    "print('Score (scaled, standardized):', round(scores_scaled_std.mean(), 4))\n",
    "print('Score (scaled, normalized):', round(scores_scaled_norm.mean(), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these first results, we'd go for normalized data!  \n",
    "But can we improve even better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most models have many parameters that work better with some datasets than with others. Same goes with the parameters from regularization which we learned that are selected based on a trial and error process. So how do we deal selecting the parameter values that work best for our data?\n",
    "\n",
    "#### GridSearchCV\n",
    "\n",
    "[Grid search](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) is a tuning technique that attempts to compute the optimum values of hyperparameters. It performs an exhaustive search over a prior defined parameter space using cross-validation (hence the **CV** suffix). That means it will evaluate all of the possible parameter combinations of the search space in order to find and return the best combination. \n",
    "\n",
    "\n",
    "This task, however, starts to become very time-consuming if there are many hyperparameters and the search space is huge. As you can see for k= 5 and for 2 parameters with 2, and respectively 3 values, thus 6 combinations, the GridSearcCV runs 30 modeling steps in order to just come up with the best values for the two parameters.\n",
    "\n",
    "![grid search](images/grid_search_cv.png)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the possible parameters of the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what parameters does sklearn.linear_model.LogisticRegression() have?\n",
    "logreg_classifier.get_params()#.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose parameters and values for grid search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining parameter grid (as dictionary)\n",
    "param_grid = {\"solver\" : [\"lbfgs\", \"liblinear\", \"newton-cg\", \"newton-cholesky\", \"sag\", \"saga\"],\n",
    "              \"penalty\" : [\"l2\", \"l1\", \"elasticnet\"],\n",
    "              \"fit_intercept\" : [True, False],\n",
    "              \"C\" : [1, 0.9, 0.8],\n",
    "              \"multi_class\" : [\"auto\", \"ovr\", \"multinomial\"]\n",
    "             }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying chosen parameter values in grid search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Instantiate gridsearch and define the metric to optimize \n",
    "gs = GridSearchCV(logreg_classifier, param_grid, scoring='accuracy',\n",
    "                  cv=5, verbose=1)\n",
    "\n",
    "# Fit gridsearch object to data. Also lets see how long it takes.\n",
    "start = timer()\n",
    "gs.fit(X2_train_preprocessed, y_train)\n",
    "end = timer()\n",
    "gs_time = end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best score\n",
    "print('Best score:', round(gs.best_score_, 3))\n",
    "print('In comparison: Score (scaled, normalized):', round(scores_scaled_norm.mean(), 3)) #cross validation score from normalized data\n",
    "\n",
    "# Best parameters\n",
    "print('Best parameters:', gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Grid search score: 0.793\n",
    "- Un-tuned (scaled, normalized) score: 0.792"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we've trained the model and validated it - now let's put it to test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning the fitted LogRegClassifier model with best parameter combination to a new variable logreg_best\n",
    "logreg_best = gs.best_estimator_\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred_test_gs = logreg_best.predict(X2_test_preprocessed)\n",
    "\n",
    "# Let us print out the performance of our model on the test set.\n",
    "gs_accuracy = accuracy_score(y_test, y_pred_test_gs)\n",
    "print('Test accuracy: {:2f}'.format(gs_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)\n",
    "\n",
    "As an alternative to grid search we can use sklearn's RandomizedSearchCV(). Random search will not try every possible combination of our search space but will randomly pick and evaluate parameter combinations.  \n",
    "\n",
    "Applying chosen parameter values in randomized search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate random search and define the metric to optimize \n",
    "rs = RandomizedSearchCV(logreg_classifier, param_grid, scoring='accuracy',\n",
    "                  cv=5, verbose=1, n_jobs=-1, n_iter=3)\n",
    "\n",
    "# Fit randomized search object to data\n",
    "start = timer()\n",
    "rs.fit(X2_train_preprocessed, y_train)\n",
    "end = timer()\n",
    "rgs_time = end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best score\n",
    "print('Best score:', round(rs.best_score_, 3))\n",
    "print('In comparison: Score (scaled, normalized):', round(scores_scaled_norm.mean(), 3)) #cross validation score from normalized data\n",
    "\n",
    "# Best parameters\n",
    "print('Best parameters:', rs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning the fitted SGDClassifier model with best parameter combination to a new variable sgd_best\n",
    "logreg_best_rs = rs.best_estimator_\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred_test_rs = logreg_best_rs.predict(X2_test_preprocessed)\n",
    "\n",
    "\n",
    "# Let us print out the performance of our model on the test set.\n",
    "rs_accuracy = accuracy_score(y_test, y_pred_test_rs)\n",
    "print('Test accuracy: {:2f}'.format(rs_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Grid search took {gs_time} seconds to run with accuracy: {gs_accuracy:f}\")\n",
    "print(f\"Randomized Grid search took {rgs_time} seconds to run with accuracy: {rs_accuracy:f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix for the grid search\n",
    "cm_final = confusion_matrix(y_test, y_pred_test_gs)\n",
    "cm_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix for the testes/validated model after parameter tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "group_counts = ['{0:0.0f}'.format(value) for value in\n",
    "                cm_final.flatten()]\n",
    "group_percentages = ['{0:.2%}'.format(value) for value in\n",
    "                     cm_final.flatten()/np.sum(cm_final)]\n",
    "labels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in\n",
    "          zip(group_names,group_counts,group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "sns.heatmap(cm_final, annot=labels, fmt='', cmap='Blues');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In comparison: The initial first, more unreliable (hence not crossfold validated) model results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "group_counts = ['{0:0.0f}'.format(value) for value in\n",
    "                cm.flatten()]\n",
    "group_percentages = ['{0:.2%}'.format(value) for value in\n",
    "                     cm.flatten()/np.sum(cm)]\n",
    "labels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in\n",
    "          zip(group_names,group_counts,group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "sns.heatmap(cm, annot=labels, fmt='', cmap='Blues');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix for grid search model\n",
    "confusion_matrix(y_test, y_pred_test_gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix for random search model\n",
    "confusion_matrix(y_test, y_pred_test_rs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nf_sql2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
